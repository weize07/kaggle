{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weize/workspace/python/VENV-3.5.2/lib/python3.5/site-packages/ipykernel_launcher.py:29: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare_range</th>\n",
       "      <th>Age_range</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 11]</td>\n",
       "      <td>(18, 50]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>(51, 101]</td>\n",
       "      <td>(18, 50]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 11]</td>\n",
       "      <td>(18, 50]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(51, 101]</td>\n",
       "      <td>(18, 50]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 11]</td>\n",
       "      <td>(18, 50]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex  SibSp  Parch  Embarked Fare_range Age_range  Title\n",
       "0         0       3    1      1      0         0    (0, 11]  (18, 50]      2\n",
       "1         1       1    0      1      0         1  (51, 101]  (18, 50]      3\n",
       "2         1       3    0      0      0         0    (0, 11]  (18, 50]      1\n",
       "3         1       1    0      1      0         0  (51, 101]  (18, 50]      3\n",
       "4         0       3    1      0      0         0    (0, 11]  (18, 50]      2"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('./data/train.csv')\n",
    "ds.head()\n",
    "# ds.groupby(['Embarked', 'Survived'])['Embarked'].count()\n",
    "# ds = ds.fillna(ds.mean())\n",
    "# ds['Age_10'] = ds['Age'] // 10\n",
    "# ds['Fare_5'] = ds['Fare'] // 5\n",
    "# ds = ds.drop(['Name','Cabin','Ticket', 'Age', 'Fare', 'PassengerId'], 1)\n",
    "\n",
    "ds['Embarked'] = ds['Embarked'].fillna('S')\n",
    "\n",
    "cleanup_nums = {\"Sex\":     {\"male\": 1, \"female\": 0},\n",
    "                \"Embarked\": {\"S\": 0, \"C\": 1, \"Q\": 2}}\n",
    "ds.replace(cleanup_nums, inplace=True)\n",
    "# ds = ds.fillna(ds.median())\n",
    "# ds\n",
    "\n",
    "\n",
    "ds.groupby(ds['Fare']//10)['Fare'].count()\n",
    "fare_bins = [0,11,51,101,201,1000]\n",
    "ds['Fare_range'] = pd.cut(ds['Fare'], fare_bins)\n",
    "ds['Fare_range'] = ds['Fare_range'].fillna(pd.Interval(11, 51, closed='right'))\n",
    "\n",
    "age_bins = [0,3,10,18,50,60,120]\n",
    "ds['Age_range'] = pd.cut(ds['Age'], age_bins)\n",
    "ds['Age_range'].unique()\n",
    "ds.groupby(ds['Age_range'])['Age_range'].count()\n",
    "ds['Age_range'] = ds['Age_range'].fillna(pd.Interval(18, 50, closed='right'))\n",
    "\n",
    "ds['Title'] = ds['Name'].str.extract(r'([A-Z][a-z]+\\.)')\n",
    "ds = ds.drop(['PassengerId', 'Ticket', 'Cabin', 'Age', 'Fare', 'Name'], 1)\n",
    "\n",
    "title_dict = {'Capt.': 'officer', 'Col.': 'officer', 'Countess.': 'royal', \n",
    "              'Don.':'royal', 'Dr.': 'officer', 'Jonkheer.': 'royal',\n",
    "              'Lady.': 'royal', 'Major.': 'officer', 'Master.': 'Master', 'Miss.': 'Miss', \n",
    "             'Mlle.': 'Miss', 'Mme.': 'Mrs', 'Mr.': 'Mr', 'Mrs.': 'Mrs',\n",
    "             'Ms.': 'Mrs', 'Rev.': 'officer', 'Sir.': 'royal'}\n",
    "ds.replace({'Title': title_dict}, inplace=True)\n",
    "title_dict2 = {'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'officer': 4, 'royal': 5}\n",
    "ds.replace({'Title': title_dict2}, inplace=True)\n",
    "ds.head()\n",
    "# ds.groupby('Title')['Title'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_ranges = sorted(ds['Fare_range'].unique())\n",
    "fare_dict = {}\n",
    "for i in range(len(fare_ranges)):\n",
    "    fare_dict[fare_ranges[i]] = i\n",
    "Age_dict = {}\n",
    "Age_ranges = sorted(ds['Age_range'].unique())\n",
    "for i in range(len(Age_ranges)):\n",
    "    Age_dict[Age_ranges[i]] = i\n",
    "cleanups = {\"Age_range\":     Age_dict, 'Fare_range': fare_dict}\n",
    "ds.replace(cleanups, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_name = 'Title'\n",
    "dies = ds[ds['Survived']==0]\n",
    "dies = dies.groupby(feature_name).count()['Survived']\n",
    "dies = dies.as_matrix()\n",
    "alive = ds[ds['Survived']==1]\n",
    "alive = alive.groupby(feature_name).count()['Survived']\n",
    "alive = alive.as_matrix()\n",
    "name_list = sorted(ds[feature_name].unique())\n",
    "(dies, alive)\n",
    "\n",
    "x = list(range(len(dies)))\n",
    "total_width, n = 0.8, 2\n",
    "width = total_width / n\n",
    " \n",
    "plt.bar(x, dies, width=width, label='die',fc = 'y')\n",
    "for i in range(len(x)):\n",
    "    x[i] = x[i] + width\n",
    "plt.bar(x, alive, width=width, label='alive',tick_label = name_list,fc = 'r')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weize/workspace/python/VENV-3.5.2/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8208955223880597\n",
      "0.7910447761194029\n",
      "0.8059701492537313\n",
      "0.8022388059701493\n",
      "0.8619402985074627\n",
      "0.8470149253731343\n",
      "0.8582089552238806\n",
      "0.7761194029850746\n",
      "0.832089552238806\n",
      "0.8022388059701493\n",
      "0.832089552238806\n",
      "0.8694029850746269\n",
      "0.8097014925373134\n",
      "0.8134328358208955\n",
      "0.8507462686567164\n",
      "0.832089552238806\n",
      "0.8208955223880597\n",
      "0.7947761194029851\n",
      "0.835820895522388\n",
      "0.835820895522388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = ds.as_matrix()\n",
    "Y = arr[:,:1]\n",
    "Y\n",
    "X = arr[:,1:]\n",
    "# X = preprocessing.scale(X)\n",
    "for random_state in range(20):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.3, random_state=random_state)\n",
    "    clf = svm.SVC(kernel='rbf', C=0.5).fit(X_train, y_train)\n",
    "    print(clf.score(X_val, y_val))\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived\n",
      "892 , 0\n",
      "893 , 1\n",
      "894 , 0\n",
      "895 , 0\n",
      "896 , 1\n",
      "897 , 0\n",
      "898 , 1\n",
      "899 , 0\n",
      "900 , 1\n",
      "901 , 0\n",
      "902 , 0\n",
      "903 , 0\n",
      "904 , 1\n",
      "905 , 0\n",
      "906 , 1\n",
      "907 , 1\n",
      "908 , 0\n",
      "909 , 0\n",
      "910 , 1\n",
      "911 , 1\n",
      "912 , 0\n",
      "913 , 1\n",
      "914 , 1\n",
      "915 , 0\n",
      "916 , 1\n",
      "917 , 0\n",
      "918 , 1\n",
      "919 , 0\n",
      "920 , 0\n",
      "921 , 0\n",
      "922 , 0\n",
      "923 , 0\n",
      "924 , 1\n",
      "925 , 1\n",
      "926 , 0\n",
      "927 , 0\n",
      "928 , 1\n",
      "929 , 1\n",
      "930 , 0\n",
      "931 , 0\n",
      "932 , 0\n",
      "933 , 0\n",
      "934 , 0\n",
      "935 , 1\n",
      "936 , 1\n",
      "937 , 0\n",
      "938 , 0\n",
      "939 , 0\n",
      "940 , 1\n",
      "941 , 1\n",
      "942 , 0\n",
      "943 , 0\n",
      "944 , 1\n",
      "945 , 1\n",
      "946 , 0\n",
      "947 , 0\n",
      "948 , 0\n",
      "949 , 0\n",
      "950 , 0\n",
      "951 , 1\n",
      "952 , 0\n",
      "953 , 0\n",
      "954 , 0\n",
      "955 , 1\n",
      "956 , 1\n",
      "957 , 1\n",
      "958 , 1\n",
      "959 , 0\n",
      "960 , 0\n",
      "961 , 0\n",
      "962 , 1\n",
      "963 , 0\n",
      "964 , 1\n",
      "965 , 0\n",
      "966 , 1\n",
      "967 , 0\n",
      "968 , 0\n",
      "969 , 1\n",
      "970 , 0\n",
      "971 , 1\n",
      "972 , 1\n",
      "973 , 0\n",
      "974 , 0\n",
      "975 , 0\n",
      "976 , 0\n",
      "977 , 0\n",
      "978 , 1\n",
      "979 , 1\n",
      "980 , 1\n",
      "981 , 1\n",
      "982 , 1\n",
      "983 , 0\n",
      "984 , 1\n",
      "985 , 0\n",
      "986 , 0\n",
      "987 , 0\n",
      "988 , 1\n",
      "989 , 0\n",
      "990 , 1\n",
      "991 , 0\n",
      "992 , 1\n",
      "993 , 0\n",
      "994 , 0\n",
      "995 , 0\n",
      "996 , 1\n",
      "997 , 0\n",
      "998 , 0\n",
      "999 , 0\n",
      "1000 , 0\n",
      "1001 , 0\n",
      "1002 , 0\n",
      "1003 , 1\n",
      "1004 , 1\n",
      "1005 , 1\n",
      "1006 , 1\n",
      "1007 , 0\n",
      "1008 , 0\n",
      "1009 , 1\n",
      "1010 , 0\n",
      "1011 , 1\n",
      "1012 , 1\n",
      "1013 , 0\n",
      "1014 , 1\n",
      "1015 , 0\n",
      "1016 , 0\n",
      "1017 , 1\n",
      "1018 , 0\n",
      "1019 , 1\n",
      "1020 , 0\n",
      "1021 , 0\n",
      "1022 , 0\n",
      "1023 , 0\n",
      "1024 , 0\n",
      "1025 , 0\n",
      "1026 , 0\n",
      "1027 , 0\n",
      "1028 , 0\n",
      "1029 , 0\n",
      "1030 , 1\n",
      "1031 , 0\n",
      "1032 , 0\n",
      "1033 , 1\n",
      "1034 , 0\n",
      "1035 , 0\n",
      "1036 , 0\n",
      "1037 , 0\n",
      "1038 , 0\n",
      "1039 , 0\n",
      "1040 , 0\n",
      "1041 , 0\n",
      "1042 , 1\n",
      "1043 , 0\n",
      "1044 , 0\n",
      "1045 , 1\n",
      "1046 , 0\n",
      "1047 , 0\n",
      "1048 , 1\n",
      "1049 , 1\n",
      "1050 , 0\n",
      "1051 , 1\n",
      "1052 , 1\n",
      "1053 , 1\n",
      "1054 , 1\n",
      "1055 , 0\n",
      "1056 , 0\n",
      "1057 , 1\n",
      "1058 , 0\n",
      "1059 , 0\n",
      "1060 , 1\n",
      "1061 , 1\n",
      "1062 , 0\n",
      "1063 , 0\n",
      "1064 , 0\n",
      "1065 , 0\n",
      "1066 , 0\n",
      "1067 , 1\n",
      "1068 , 1\n",
      "1069 , 0\n",
      "1070 , 1\n",
      "1071 , 1\n",
      "1072 , 0\n",
      "1073 , 0\n",
      "1074 , 1\n",
      "1075 , 0\n",
      "1076 , 1\n",
      "1077 , 0\n",
      "1078 , 1\n",
      "1079 , 0\n",
      "1080 , 0\n",
      "1081 , 0\n",
      "1082 , 0\n",
      "1083 , 0\n",
      "1084 , 1\n",
      "1085 , 0\n",
      "1086 , 1\n",
      "1087 , 0\n",
      "1088 , 1\n",
      "1089 , 1\n",
      "1090 , 0\n",
      "1091 , 1\n",
      "1092 , 1\n",
      "1093 , 1\n",
      "1094 , 1\n",
      "1095 , 1\n",
      "1096 , 0\n",
      "1097 , 0\n",
      "1098 , 1\n",
      "1099 , 0\n",
      "1100 , 1\n",
      "1101 , 0\n",
      "1102 , 0\n",
      "1103 , 0\n",
      "1104 , 0\n",
      "1105 , 1\n",
      "1106 , 0\n",
      "1107 , 0\n",
      "1108 , 1\n",
      "1109 , 0\n",
      "1110 , 1\n",
      "1111 , 0\n",
      "1112 , 1\n",
      "1113 , 0\n",
      "1114 , 1\n",
      "1115 , 0\n",
      "1116 , 1\n",
      "1117 , 1\n",
      "1118 , 0\n",
      "1119 , 1\n",
      "1120 , 0\n",
      "1121 , 0\n",
      "1122 , 0\n",
      "1123 , 1\n",
      "1124 , 0\n",
      "1125 , 0\n",
      "1126 , 0\n",
      "1127 , 0\n",
      "1128 , 0\n",
      "1129 , 0\n",
      "1130 , 1\n",
      "1131 , 1\n",
      "1132 , 1\n",
      "1133 , 1\n",
      "1134 , 0\n",
      "1135 , 0\n",
      "1136 , 0\n",
      "1137 , 0\n",
      "1138 , 1\n",
      "1139 , 0\n",
      "1140 , 1\n",
      "1141 , 1\n",
      "1142 , 1\n",
      "1143 , 0\n",
      "1144 , 0\n",
      "1145 , 0\n",
      "1146 , 0\n",
      "1147 , 0\n",
      "1148 , 0\n",
      "1149 , 0\n",
      "1150 , 1\n",
      "1151 , 0\n",
      "1152 , 0\n",
      "1153 , 0\n",
      "1154 , 1\n",
      "1155 , 1\n",
      "1156 , 0\n",
      "1157 , 0\n",
      "1158 , 0\n",
      "1159 , 0\n",
      "1160 , 1\n",
      "1161 , 0\n",
      "1162 , 0\n",
      "1163 , 0\n",
      "1164 , 1\n",
      "1165 , 1\n",
      "1166 , 0\n",
      "1167 , 1\n",
      "1168 , 0\n",
      "1169 , 0\n",
      "1170 , 0\n",
      "1171 , 0\n",
      "1172 , 1\n",
      "1173 , 1\n",
      "1174 , 1\n",
      "1175 , 1\n",
      "1176 , 1\n",
      "1177 , 0\n",
      "1178 , 0\n",
      "1179 , 0\n",
      "1180 , 0\n",
      "1181 , 0\n",
      "1182 , 0\n",
      "1183 , 1\n",
      "1184 , 0\n",
      "1185 , 0\n",
      "1186 , 0\n",
      "1187 , 0\n",
      "1188 , 1\n",
      "1189 , 0\n",
      "1190 , 0\n",
      "1191 , 0\n",
      "1192 , 0\n",
      "1193 , 0\n",
      "1194 , 0\n",
      "1195 , 0\n",
      "1196 , 1\n",
      "1197 , 1\n",
      "1198 , 0\n",
      "1199 , 1\n",
      "1200 , 0\n",
      "1201 , 1\n",
      "1202 , 0\n",
      "1203 , 0\n",
      "1204 , 0\n",
      "1205 , 1\n",
      "1206 , 1\n",
      "1207 , 1\n",
      "1208 , 0\n",
      "1209 , 0\n",
      "1210 , 0\n",
      "1211 , 0\n",
      "1212 , 0\n",
      "1213 , 0\n",
      "1214 , 0\n",
      "1215 , 0\n",
      "1216 , 1\n",
      "1217 , 0\n",
      "1218 , 1\n",
      "1219 , 0\n",
      "1220 , 0\n",
      "1221 , 0\n",
      "1222 , 1\n",
      "1223 , 0\n",
      "1224 , 0\n",
      "1225 , 1\n",
      "1226 , 0\n",
      "1227 , 0\n",
      "1228 , 0\n",
      "1229 , 0\n",
      "1230 , 0\n",
      "1231 , 1\n",
      "1232 , 0\n",
      "1233 , 0\n",
      "1234 , 0\n",
      "1235 , 1\n",
      "1236 , 0\n",
      "1237 , 1\n",
      "1238 , 0\n",
      "1239 , 1\n",
      "1240 , 0\n",
      "1241 , 1\n",
      "1242 , 1\n",
      "1243 , 0\n",
      "1244 , 0\n",
      "1245 , 0\n",
      "1246 , 1\n",
      "1247 , 0\n",
      "1248 , 1\n",
      "1249 , 0\n",
      "1250 , 0\n",
      "1251 , 1\n",
      "1252 , 0\n",
      "1253 , 1\n",
      "1254 , 1\n",
      "1255 , 0\n",
      "1256 , 1\n",
      "1257 , 0\n",
      "1258 , 0\n",
      "1259 , 1\n",
      "1260 , 1\n",
      "1261 , 0\n",
      "1262 , 0\n",
      "1263 , 1\n",
      "1264 , 0\n",
      "1265 , 0\n",
      "1266 , 1\n",
      "1267 , 1\n",
      "1268 , 0\n",
      "1269 , 0\n",
      "1270 , 0\n",
      "1271 , 0\n",
      "1272 , 0\n",
      "1273 , 0\n",
      "1274 , 1\n",
      "1275 , 1\n",
      "1276 , 0\n",
      "1277 , 1\n",
      "1278 , 0\n",
      "1279 , 0\n",
      "1280 , 0\n",
      "1281 , 0\n",
      "1282 , 0\n",
      "1283 , 1\n",
      "1284 , 1\n",
      "1285 , 0\n",
      "1286 , 0\n",
      "1287 , 1\n",
      "1288 , 0\n",
      "1289 , 1\n",
      "1290 , 0\n",
      "1291 , 0\n",
      "1292 , 1\n",
      "1293 , 0\n",
      "1294 , 1\n",
      "1295 , 0\n",
      "1296 , 0\n",
      "1297 , 0\n",
      "1298 , 0\n",
      "1299 , 0\n",
      "1300 , 1\n",
      "1301 , 1\n",
      "1302 , 1\n",
      "1303 , 1\n",
      "1304 , 1\n",
      "1305 , 0\n",
      "1306 , 1\n",
      "1307 , 0\n",
      "1308 , 0\n",
      "1309 , 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weize/workspace/python/VENV-3.5.2/lib/python3.5/site-packages/ipykernel_launcher.py:18: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "ds_test = pd.read_csv('./data/test.csv')\n",
    "ds_test['Embarked'] = ds_test['Embarked'].fillna('S')\n",
    "\n",
    "cleanup_nums = {\"Sex\":     {\"male\": 1, \"female\": 0},\n",
    "                \"Embarked\": {\"S\": 0, \"C\": 1, \"Q\": 2}}\n",
    "ds_test.replace(cleanup_nums, inplace=True)\n",
    "fare_bins = [0,11,51,101,201,1000]\n",
    "ds_test['Fare_range'] = pd.cut(ds_test['Fare'], fare_bins)\n",
    "ds_test['Fare_range'] = ds_test['Fare_range'].fillna(pd.Interval(11, 51, closed='right'))\n",
    "ids = ds_test['PassengerId']\n",
    "\n",
    "age_bins = [0,3,10,18,50,60,120]\n",
    "ds_test['Age_range'] = pd.cut(ds_test['Age'], age_bins)\n",
    "ds_test['Age_range'].unique()\n",
    "ds_test.groupby(ds_test['Age_range'])['Age_range'].count()\n",
    "ds_test['Age_range'] = ds_test['Age_range'].fillna(pd.Interval(18, 50, closed='right'))\n",
    "\n",
    "ds_test['Title'] = ds_test['Name'].str.extract(r'([A-Z][a-z]+\\.)')\n",
    "title_dict = {'Capt.': 'officer', 'Col.': 'officer', 'Countess.': 'royal', \n",
    "              'Don.':'royal', 'Dona.':'royal', 'Dr.': 'officer', 'Jonkheer.': 'royal',\n",
    "              'Lady.': 'royal', 'Major.': 'officer', 'Master.': 'Master', 'Miss.': 'Miss', \n",
    "             'Mlle.': 'Miss', 'Mme.': 'Mrs', 'Mr.': 'Mr', 'Mrs.': 'Mrs',\n",
    "             'Ms.': 'Mrs', 'Rev.': 'officer', 'Sir.': 'royal'}\n",
    "ds_test.replace({'Title': title_dict}, inplace=True)\n",
    "title_dict2 = {'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'officer': 4, 'royal': 5}\n",
    "ds_test.replace({'Title': title_dict2}, inplace=True)\n",
    "\n",
    "ds_test = ds_test.drop(['PassengerId', 'Ticket', 'Cabin', 'Age', 'Fare', 'Name'], 1)\n",
    "\n",
    "fare_ranges = sorted(ds_test['Fare_range'].unique())\n",
    "fare_dict = {}\n",
    "for i in range(len(fare_ranges)):\n",
    "    fare_dict[fare_ranges[i]] = i\n",
    "Age_dict = {}\n",
    "Age_ranges = sorted(ds_test['Age_range'].unique())\n",
    "for i in range(len(Age_ranges)):\n",
    "    Age_dict[Age_ranges[i]] = i\n",
    "cleanups = {\"Age_range\": Age_dict, 'Fare_range': fare_dict}\n",
    "ds_test.replace(cleanups, inplace=True)\n",
    "\n",
    "X_test = ds_test.as_matrix()\n",
    "# X_test = preprocessing.scale(X_test)\n",
    "X_test\n",
    "Y_test = clf.predict(X_test)\n",
    "Y_test\n",
    "print('PassengerId,Survived')\n",
    "for i in range(len(ids)):\n",
    "    print(ids[i], ',', int(Y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/100\n",
      "623/623 [==============================] - 0s 610us/step - loss: 2.1565 - acc: 0.5987 - val_loss: 2.2355 - val_acc: 0.6940\n",
      "Epoch 2/100\n",
      "623/623 [==============================] - 0s 72us/step - loss: 2.2060 - acc: 0.6854 - val_loss: 1.6784 - val_acc: 0.7052\n",
      "Epoch 3/100\n",
      "623/623 [==============================] - 0s 73us/step - loss: 1.8255 - acc: 0.4928 - val_loss: 1.0931 - val_acc: 0.6045\n",
      "Epoch 4/100\n",
      "623/623 [==============================] - 0s 90us/step - loss: 0.7140 - acc: 0.6806 - val_loss: 0.5831 - val_acc: 0.7425\n",
      "Epoch 5/100\n",
      "623/623 [==============================] - 0s 78us/step - loss: 0.5829 - acc: 0.7303 - val_loss: 0.5969 - val_acc: 0.7537\n",
      "Epoch 6/100\n",
      "623/623 [==============================] - 0s 66us/step - loss: 0.6450 - acc: 0.7352 - val_loss: 0.6843 - val_acc: 0.6679\n",
      "Epoch 7/100\n",
      "623/623 [==============================] - 0s 95us/step - loss: 0.6239 - acc: 0.7705 - val_loss: 1.4888 - val_acc: 0.6828\n",
      "Epoch 8/100\n",
      "623/623 [==============================] - 0s 80us/step - loss: 0.7069 - acc: 0.7432 - val_loss: 0.5219 - val_acc: 0.7873\n",
      "Epoch 9/100\n",
      "623/623 [==============================] - 0s 106us/step - loss: 0.5825 - acc: 0.7640 - val_loss: 0.5248 - val_acc: 0.8097\n",
      "Epoch 10/100\n",
      "623/623 [==============================] - 0s 67us/step - loss: 0.5739 - acc: 0.7528 - val_loss: 0.5313 - val_acc: 0.8060\n",
      "Epoch 11/100\n",
      "623/623 [==============================] - 0s 91us/step - loss: 0.6051 - acc: 0.7608 - val_loss: 0.5768 - val_acc: 0.7836\n",
      "Epoch 12/100\n",
      "623/623 [==============================] - 0s 63us/step - loss: 0.5380 - acc: 0.7785 - val_loss: 0.4874 - val_acc: 0.8022\n",
      "Epoch 13/100\n",
      "623/623 [==============================] - 0s 73us/step - loss: 0.5200 - acc: 0.7673 - val_loss: 0.4654 - val_acc: 0.8022\n",
      "Epoch 14/100\n",
      "623/623 [==============================] - 0s 81us/step - loss: 0.5498 - acc: 0.7721 - val_loss: 0.5282 - val_acc: 0.8060\n",
      "Epoch 15/100\n",
      "623/623 [==============================] - 0s 85us/step - loss: 0.6017 - acc: 0.7769 - val_loss: 0.5160 - val_acc: 0.7948\n",
      "Epoch 16/100\n",
      "623/623 [==============================] - 0s 82us/step - loss: 0.5481 - acc: 0.7576 - val_loss: 0.5506 - val_acc: 0.8097\n",
      "Epoch 17/100\n",
      "623/623 [==============================] - 0s 93us/step - loss: 0.4998 - acc: 0.7913 - val_loss: 0.4637 - val_acc: 0.7985\n",
      "Epoch 18/100\n",
      "623/623 [==============================] - 0s 74us/step - loss: 0.4924 - acc: 0.7881 - val_loss: 0.5285 - val_acc: 0.7910\n",
      "Epoch 19/100\n",
      "623/623 [==============================] - 0s 83us/step - loss: 0.5313 - acc: 0.7785 - val_loss: 0.5125 - val_acc: 0.8022\n",
      "Epoch 20/100\n",
      "623/623 [==============================] - 0s 64us/step - loss: 0.6914 - acc: 0.7689 - val_loss: 0.9829 - val_acc: 0.7649\n",
      "Epoch 21/100\n",
      "623/623 [==============================] - 0s 86us/step - loss: 0.6387 - acc: 0.7608 - val_loss: 0.5133 - val_acc: 0.7985\n",
      "Epoch 22/100\n",
      "623/623 [==============================] - 0s 82us/step - loss: 0.5930 - acc: 0.7673 - val_loss: 0.5213 - val_acc: 0.8134\n",
      "Epoch 23/100\n",
      "623/623 [==============================] - 0s 81us/step - loss: 0.5126 - acc: 0.7929 - val_loss: 0.8413 - val_acc: 0.7724\n",
      "Epoch 24/100\n",
      "623/623 [==============================] - 0s 71us/step - loss: 0.6818 - acc: 0.7769 - val_loss: 0.4487 - val_acc: 0.8097\n",
      "Epoch 25/100\n",
      "623/623 [==============================] - 0s 96us/step - loss: 0.5792 - acc: 0.7785 - val_loss: 1.0805 - val_acc: 0.7425\n",
      "Epoch 26/100\n",
      "623/623 [==============================] - 0s 77us/step - loss: 0.5288 - acc: 0.7865 - val_loss: 0.4528 - val_acc: 0.8022\n",
      "Epoch 27/100\n",
      "623/623 [==============================] - 0s 99us/step - loss: 0.5104 - acc: 0.7753 - val_loss: 0.5164 - val_acc: 0.8134\n",
      "Epoch 28/100\n",
      "623/623 [==============================] - 0s 77us/step - loss: 0.5240 - acc: 0.7769 - val_loss: 0.5507 - val_acc: 0.7836\n",
      "Epoch 29/100\n",
      "623/623 [==============================] - 0s 88us/step - loss: 0.5053 - acc: 0.7865 - val_loss: 0.5065 - val_acc: 0.8097\n",
      "Epoch 30/100\n",
      "623/623 [==============================] - 0s 103us/step - loss: 0.6128 - acc: 0.7801 - val_loss: 0.4989 - val_acc: 0.8134\n",
      "Epoch 31/100\n",
      "623/623 [==============================] - 0s 84us/step - loss: 0.5150 - acc: 0.7865 - val_loss: 0.4592 - val_acc: 0.7985\n",
      "Epoch 32/100\n",
      "623/623 [==============================] - 0s 81us/step - loss: 0.6802 - acc: 0.7753 - val_loss: 0.7819 - val_acc: 0.8022\n",
      "Epoch 33/100\n",
      "623/623 [==============================] - 0s 88us/step - loss: 0.5794 - acc: 0.7865 - val_loss: 0.4878 - val_acc: 0.8097\n",
      "Epoch 34/100\n",
      "623/623 [==============================] - 0s 88us/step - loss: 0.5377 - acc: 0.7801 - val_loss: 0.4723 - val_acc: 0.8246\n",
      "Epoch 35/100\n",
      "623/623 [==============================] - 0s 86us/step - loss: 0.4982 - acc: 0.7801 - val_loss: 0.5325 - val_acc: 0.8134\n",
      "Epoch 36/100\n",
      "623/623 [==============================] - 0s 121us/step - loss: 0.5286 - acc: 0.7721 - val_loss: 0.7163 - val_acc: 0.7575\n",
      "Epoch 37/100\n",
      "623/623 [==============================] - 0s 93us/step - loss: 0.5078 - acc: 0.7865 - val_loss: 0.5103 - val_acc: 0.8209\n",
      "Epoch 38/100\n",
      "623/623 [==============================] - 0s 82us/step - loss: 0.5044 - acc: 0.7705 - val_loss: 0.4964 - val_acc: 0.8060\n",
      "Epoch 39/100\n",
      "623/623 [==============================] - 0s 62us/step - loss: 0.5135 - acc: 0.7785 - val_loss: 0.5023 - val_acc: 0.8022\n",
      "Epoch 40/100\n",
      "623/623 [==============================] - 0s 74us/step - loss: 0.5917 - acc: 0.7753 - val_loss: 0.5451 - val_acc: 0.8134\n",
      "Epoch 41/100\n",
      "623/623 [==============================] - 0s 63us/step - loss: 0.5609 - acc: 0.7785 - val_loss: 0.5751 - val_acc: 0.8060\n",
      "Epoch 42/100\n",
      "623/623 [==============================] - 0s 68us/step - loss: 0.5865 - acc: 0.7881 - val_loss: 0.7195 - val_acc: 0.7724\n",
      "Epoch 43/100\n",
      "623/623 [==============================] - 0s 71us/step - loss: 0.5041 - acc: 0.7897 - val_loss: 0.5249 - val_acc: 0.7985\n",
      "Epoch 44/100\n",
      "623/623 [==============================] - 0s 69us/step - loss: 0.5144 - acc: 0.7769 - val_loss: 0.5186 - val_acc: 0.8172\n",
      "Epoch 45/100\n",
      "623/623 [==============================] - 0s 72us/step - loss: 0.5241 - acc: 0.7833 - val_loss: 0.5586 - val_acc: 0.7015\n",
      "Epoch 46/100\n",
      "623/623 [==============================] - 0s 74us/step - loss: 0.6164 - acc: 0.7817 - val_loss: 0.8067 - val_acc: 0.8097\n",
      "Epoch 47/100\n",
      "623/623 [==============================] - 0s 73us/step - loss: 0.5870 - acc: 0.7785 - val_loss: 0.5955 - val_acc: 0.8022\n",
      "Epoch 48/100\n",
      "623/623 [==============================] - 0s 75us/step - loss: 0.4878 - acc: 0.7913 - val_loss: 0.5298 - val_acc: 0.8097\n",
      "Epoch 49/100\n",
      "623/623 [==============================] - 0s 72us/step - loss: 0.4776 - acc: 0.7865 - val_loss: 0.4363 - val_acc: 0.8134\n",
      "Epoch 50/100\n",
      "623/623 [==============================] - 0s 74us/step - loss: 0.5590 - acc: 0.7737 - val_loss: 0.4566 - val_acc: 0.8097\n",
      "Epoch 51/100\n",
      "623/623 [==============================] - 0s 67us/step - loss: 0.5918 - acc: 0.7737 - val_loss: 0.5008 - val_acc: 0.8097\n",
      "Epoch 52/100\n",
      "623/623 [==============================] - 0s 72us/step - loss: 0.5056 - acc: 0.7913 - val_loss: 0.4384 - val_acc: 0.8060\n",
      "Epoch 53/100\n",
      "623/623 [==============================] - 0s 63us/step - loss: 0.5812 - acc: 0.7705 - val_loss: 0.5098 - val_acc: 0.8134\n",
      "Epoch 54/100\n",
      "623/623 [==============================] - 0s 72us/step - loss: 0.5385 - acc: 0.7785 - val_loss: 0.4886 - val_acc: 0.8097\n",
      "Epoch 55/100\n",
      "623/623 [==============================] - 0s 62us/step - loss: 0.5430 - acc: 0.7801 - val_loss: 0.7291 - val_acc: 0.8060\n",
      "Epoch 56/100\n",
      "623/623 [==============================] - 0s 70us/step - loss: 0.5758 - acc: 0.7801 - val_loss: 0.4508 - val_acc: 0.8097\n",
      "Epoch 57/100\n",
      "623/623 [==============================] - 0s 71us/step - loss: 0.5137 - acc: 0.7721 - val_loss: 0.4461 - val_acc: 0.8134\n",
      "Epoch 58/100\n",
      "623/623 [==============================] - 0s 66us/step - loss: 0.5530 - acc: 0.7769 - val_loss: 0.5542 - val_acc: 0.7836\n",
      "Epoch 59/100\n",
      "623/623 [==============================] - 0s 66us/step - loss: 0.5605 - acc: 0.7801 - val_loss: 0.4515 - val_acc: 0.8060\n",
      "Epoch 60/100\n",
      "623/623 [==============================] - 0s 85us/step - loss: 0.6101 - acc: 0.7849 - val_loss: 0.5809 - val_acc: 0.7910\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 0s 76us/step - loss: 0.4838 - acc: 0.7865 - val_loss: 0.4923 - val_acc: 0.8172\n",
      "Epoch 62/100\n",
      "623/623 [==============================] - 0s 67us/step - loss: 0.5531 - acc: 0.7817 - val_loss: 0.4658 - val_acc: 0.8097\n",
      "Epoch 63/100\n",
      "623/623 [==============================] - 0s 65us/step - loss: 0.5950 - acc: 0.7624 - val_loss: 0.5348 - val_acc: 0.8134\n",
      "Epoch 64/100\n",
      "623/623 [==============================] - 0s 69us/step - loss: 0.5682 - acc: 0.7801 - val_loss: 0.5546 - val_acc: 0.7090\n",
      "Epoch 65/100\n",
      "623/623 [==============================] - 0s 56us/step - loss: 0.5176 - acc: 0.7865 - val_loss: 0.5498 - val_acc: 0.7836\n",
      "Epoch 66/100\n",
      "623/623 [==============================] - 0s 71us/step - loss: 0.5261 - acc: 0.7801 - val_loss: 0.5059 - val_acc: 0.8097\n",
      "Epoch 67/100\n",
      "623/623 [==============================] - 0s 63us/step - loss: 0.4852 - acc: 0.7865 - val_loss: 1.2757 - val_acc: 0.7388\n",
      "Epoch 68/100\n",
      "623/623 [==============================] - 0s 89us/step - loss: 0.5881 - acc: 0.7881 - val_loss: 0.4930 - val_acc: 0.8060\n",
      "Epoch 69/100\n",
      "623/623 [==============================] - 0s 74us/step - loss: 0.5672 - acc: 0.7945 - val_loss: 0.5092 - val_acc: 0.8134\n",
      "Epoch 70/100\n",
      "623/623 [==============================] - 0s 68us/step - loss: 0.5961 - acc: 0.7849 - val_loss: 0.5076 - val_acc: 0.8172\n",
      "Epoch 71/100\n",
      "623/623 [==============================] - 0s 64us/step - loss: 0.5603 - acc: 0.7721 - val_loss: 0.5955 - val_acc: 0.8134\n",
      "Epoch 72/100\n",
      "623/623 [==============================] - 0s 61us/step - loss: 0.6265 - acc: 0.7817 - val_loss: 0.5439 - val_acc: 0.8022\n",
      "Epoch 73/100\n",
      "623/623 [==============================] - 0s 68us/step - loss: 0.6040 - acc: 0.7801 - val_loss: 0.5325 - val_acc: 0.8134\n",
      "Epoch 74/100\n",
      "623/623 [==============================] - 0s 62us/step - loss: 0.6080 - acc: 0.7705 - val_loss: 0.5080 - val_acc: 0.8022\n",
      "Epoch 75/100\n",
      "623/623 [==============================] - 0s 62us/step - loss: 0.4957 - acc: 0.7833 - val_loss: 0.4652 - val_acc: 0.7948\n",
      "Epoch 76/100\n",
      "623/623 [==============================] - 0s 77us/step - loss: 0.6754 - acc: 0.7913 - val_loss: 0.7106 - val_acc: 0.6978\n",
      "Epoch 77/100\n",
      "623/623 [==============================] - 0s 63us/step - loss: 0.5138 - acc: 0.7817 - val_loss: 0.4618 - val_acc: 0.8172\n",
      "Epoch 78/100\n",
      "623/623 [==============================] - 0s 76us/step - loss: 0.5712 - acc: 0.7769 - val_loss: 0.5076 - val_acc: 0.8134\n",
      "Epoch 79/100\n",
      "623/623 [==============================] - 0s 66us/step - loss: 0.4836 - acc: 0.7945 - val_loss: 0.4606 - val_acc: 0.8022\n",
      "Epoch 80/100\n",
      "623/623 [==============================] - 0s 65us/step - loss: 0.5082 - acc: 0.7801 - val_loss: 0.4890 - val_acc: 0.7985\n",
      "Epoch 81/100\n",
      "623/623 [==============================] - 0s 61us/step - loss: 0.5036 - acc: 0.7881 - val_loss: 0.4757 - val_acc: 0.7873\n",
      "Epoch 82/100\n",
      "623/623 [==============================] - 0s 61us/step - loss: 0.5487 - acc: 0.7785 - val_loss: 0.5850 - val_acc: 0.7761\n",
      "Epoch 83/100\n",
      "623/623 [==============================] - 0s 61us/step - loss: 0.5273 - acc: 0.7833 - val_loss: 1.2646 - val_acc: 0.7313\n",
      "Epoch 84/100\n",
      "623/623 [==============================] - 0s 58us/step - loss: 0.5597 - acc: 0.7865 - val_loss: 0.4537 - val_acc: 0.8134\n",
      "Epoch 85/100\n",
      "623/623 [==============================] - 0s 63us/step - loss: 0.5397 - acc: 0.7721 - val_loss: 0.9438 - val_acc: 0.7873\n",
      "Epoch 86/100\n",
      "623/623 [==============================] - 0s 92us/step - loss: 0.7072 - acc: 0.7737 - val_loss: 0.5425 - val_acc: 0.8097\n",
      "Epoch 87/100\n",
      "623/623 [==============================] - 0s 62us/step - loss: 0.4925 - acc: 0.7865 - val_loss: 0.4962 - val_acc: 0.8134\n",
      "Epoch 88/100\n",
      "623/623 [==============================] - 0s 71us/step - loss: 0.6744 - acc: 0.8074 - val_loss: 0.5349 - val_acc: 0.8060\n",
      "Epoch 89/100\n",
      "623/623 [==============================] - 0s 76us/step - loss: 0.5713 - acc: 0.7785 - val_loss: 0.4934 - val_acc: 0.7910\n",
      "Epoch 90/100\n",
      "623/623 [==============================] - 0s 67us/step - loss: 0.6268 - acc: 0.7817 - val_loss: 0.5845 - val_acc: 0.8097\n",
      "Epoch 91/100\n",
      "623/623 [==============================] - 0s 61us/step - loss: 0.5756 - acc: 0.7817 - val_loss: 0.4856 - val_acc: 0.8134\n",
      "Epoch 92/100\n",
      "623/623 [==============================] - 0s 65us/step - loss: 0.7404 - acc: 0.7721 - val_loss: 0.6105 - val_acc: 0.8022\n",
      "Epoch 93/100\n",
      "623/623 [==============================] - 0s 65us/step - loss: 0.8148 - acc: 0.7528 - val_loss: 0.5148 - val_acc: 0.8097\n",
      "Epoch 94/100\n",
      "623/623 [==============================] - 0s 57us/step - loss: 0.5136 - acc: 0.7865 - val_loss: 0.5349 - val_acc: 0.8097\n",
      "Epoch 95/100\n",
      "623/623 [==============================] - 0s 63us/step - loss: 0.7043 - acc: 0.7897 - val_loss: 0.9333 - val_acc: 0.7985\n",
      "Epoch 96/100\n",
      "623/623 [==============================] - 0s 60us/step - loss: 0.6267 - acc: 0.7624 - val_loss: 0.5298 - val_acc: 0.7985\n",
      "Epoch 97/100\n",
      "623/623 [==============================] - 0s 65us/step - loss: 0.5399 - acc: 0.8010 - val_loss: 0.6867 - val_acc: 0.8134\n",
      "Epoch 98/100\n",
      "623/623 [==============================] - 0s 73us/step - loss: 0.9047 - acc: 0.7721 - val_loss: 0.5673 - val_acc: 0.7836\n",
      "Epoch 99/100\n",
      "623/623 [==============================] - 0s 74us/step - loss: 0.5215 - acc: 0.7833 - val_loss: 0.5376 - val_acc: 0.8134\n",
      "Epoch 100/100\n",
      "623/623 [==============================] - 0s 80us/step - loss: 0.5609 - acc: 0.7737 - val_loss: 0.4647 - val_acc: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f393cf2ad30>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "model = Sequential()\n",
    "# model.add(Dense(32, input_shape=(8,), kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dense(64, input_shape=(8,)))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_val, y_val))\n",
    "# model.fit(X, Y, epochs=30, batch_size=32)\n",
    "# Y_test = model.predict_classes(X_test)\n",
    "# print('PassengerId,Survived')\n",
    "# for i in range(len(ids)):\n",
    "#     print(ids[i], ',', Y_test[i][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
